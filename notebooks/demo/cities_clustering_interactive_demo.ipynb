{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2226a1-7c3a-4b84-a1f3-6ef27daa4563",
   "metadata": {},
   "source": [
    "# Clustering of Cities - interactive demo\n",
    "v0.1\n",
    "\n",
    "Authors: Maria Ricci, Manuel LÃ¶hnertz (space4environment), Mohamed-Bachir Belaid (NILU)\n",
    "\n",
    "---\n",
    "### Scope\n",
    "Run clustering analysis of ~700 European cities based on land, climate and socioeconomic indicators. Tune the different parameters to get the optimal result.\n",
    "### Data\n",
    "The collection of indicators is available ...\n",
    "TODO publish data\n",
    "### Requirements\n",
    "TODO required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232d689-a204-4fb3-bc19-ecdebf3dd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# for notebook interaction\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "# from ipywidgets import interact, fixed, interactive\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3fed8-8d0b-4234-a6f3-d22f5defa4ae",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Get data from database. \n",
    "\n",
    "Requirements: \n",
    "- `src` package installed in your conda environment. To install it, run `pip install -e .` from the root folder\n",
    "- `database.ini` configuration file to connect to the database. Alternatively, set up your own data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde36c99-d8c9-4e35-aa43-58b07fdbc507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import os\n",
    "from sqlalchemy import text\n",
    "\n",
    "required = {'src'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "if missing:\n",
    "    print('src package is not installed')\n",
    "else:\n",
    "    from src import db_connect\n",
    "    home_dir = os.environ.get('HOME') # set here your base directory/root folder\n",
    "    engine_postgresql = db_connect.create_engine(db_config = f\"{home_dir}/uc1-urban-climate/database.ini\")\n",
    "\n",
    "    with engine_postgresql.begin() as conn:\n",
    "        query = text(\"\"\"SELECT \n",
    "                    urau_code, urau_name, _wgs84x, _wgs84y, ez_code, city_area_ha, dem_mean,\n",
    "                    imd_percent_2018, treecover_percent_2018,\n",
    "                    class_11100, class_11210, class_11220, class_11230,\n",
    "                    class_11240, class_11300, class_12100, class_12210,\n",
    "                    class_12220, class_12230, class_12300, class_12400,\n",
    "                    class_13100, class_13300, class_13400, class_14100,\n",
    "                    class_14200, class_21000, class_22000, class_23000,\n",
    "                    class_24000, class_25000, class_31000, class_32000,\n",
    "                    class_33000, class_40000, class_50000, urban_blue_percent,\n",
    "                    urban_green_percent, avg_2m_temp_kelvin_2018,\n",
    "                    number_of_summer_days_2018, number_of_tropical_nights_2018,\n",
    "                    utci_heat_nights_2018, coastal_city, de1001v_2018, de1028v_2018,\n",
    "                    de1055v_2018, ec1174v_2018, ec1010v_2018, ec1020i_2018,\n",
    "                    ec3040v_2018, sa2013v_2018, de1028i_2018, de1055i_2018\n",
    "                    FROM public.city_2018_demo_view;\"\"\")\n",
    "        df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO improve table\n",
    "#TODO add link to data viz\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=[\"City name\", \"area_ha\", \"dem_mean\", \"ez_code\", \"imd_percent_2018\", \"treecover_percent_2018\", \"avg_2m_temp_kelvin_2018\", \"class_50000\"]),\n",
    "    cells=dict(values=[df.urau_name, df.city_area_ha, df.dem_mean, df.ez_code, df.imd_percent_2018, df.treecover_percent_2018, df.avg_2m_temp_kelvin_2018, df.class_50000]))\n",
    "])\n",
    "\n",
    "print(f'There are {df.shape[0]} cities and {df.shape[1]} indicators')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102a76d",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "First check how many missing values there are for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b439dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gaps = df.isna().sum()\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=[\"Attribute\", \"Count of NoData\", \"Mean\", \"Variance\"]),\n",
    "    cells=dict(values=[df.columns[5:], df_gaps[5:], df.mean(axis=0, numeric_only=True)[2:], df.var(axis=0, numeric_only=True)[2:]]))\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971fba7",
   "metadata": {},
   "source": [
    "### Treat missing values\n",
    "There are two methods to treat missing values:\n",
    "- `remove` the cities with missing values\n",
    "- `impute` the missing values with a computed value, e.g. the average\n",
    "\n",
    "If a feature has too many missing values, imputation does not work, and removing rows leads to a significant dataset reduction. In this case, we remove the feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647d016-840c-4b95-9893-b75c39f7e3e5",
   "metadata": {},
   "source": [
    "### Treat categorical features\n",
    "There are three methods to deal with categorical features:\n",
    "- `label_encoding` assigns a numerical value to each category. #TODO explain pro/contra\n",
    "- `binary_encoding` turns each category into a new binary feature #TODO explain pro/contra\n",
    "- `remove` the categorical feature altogether. May be used later for result interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d094ac-d75f-40d0-b36d-9aac21bfeffe",
   "metadata": {},
   "source": [
    "### Correlation analysis\n",
    "Simple correlation anaylsis of the numerical features. When two features are highly correlated, there is no need to keep them both for cluster analysis. We remove features that have a correlation >0.9 (<-0.9 resp.) to another feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42512b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis\n",
    "# https://plotly.com/python/figurewidget-app/\n",
    "def remove_features(df, min_cities = 100):\n",
    "    df_gaps = df.isna().sum()\n",
    "    columns_to_drop = df_gaps[df_gaps>min_cities].index.to_list()\n",
    "    new_df = df.drop(columns=columns_to_drop)\n",
    "    return new_df\n",
    "\n",
    "def treat_missing_values(df, method='remove'):\n",
    "    if method=='remove':\n",
    "        new_df = df.dropna(axis='index')\n",
    "    else:\n",
    "        new_df = df.fillna(df.mean(numeric_only=True))\n",
    "    return new_df\n",
    "\n",
    "def treat_cat_features(df, method='binary_encoding', feature='ez_code'):\n",
    "    if method=='binary_encoding':\n",
    "        new_df = pd.get_dummies(df, columns=[feature])\n",
    "    elif method=='label_encoding':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df[feature])\n",
    "        new_df = df\n",
    "        new_df[feature] = le.transform(df[feature])\n",
    "    else:\n",
    "        new_df = df.drop(columns=[feature])\n",
    "    return new_df\n",
    "\n",
    "def get_correlated_pairs(df, threshold=0.9):\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "    # create mask to display only upper left triangle \n",
    "    mask = np.zeros_like(correlation_matrix, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cm_abs = np.abs(correlation_matrix.mask(mask))\n",
    "    correlated_attrs = cm_abs.where(cm_abs>threshold).stack()\n",
    "    correlated_pairs = correlated_attrs.index.to_list()\n",
    "    val1 = list(set([val for val,_ in correlated_pairs]))\n",
    "    new_df = df.drop(columns=val1)\n",
    "    return correlation_matrix, mask, correlated_pairs, new_df\n",
    "\n",
    "def data_preparation(df, min_cities, missing_val_method, cat_features_method, cat_feature_name, threshold):\n",
    "    df1 = remove_features(df, min_cities)\n",
    "    df2 = treat_missing_values(df1, missing_val_method)\n",
    "    df3 = treat_cat_features(df2, cat_features_method, cat_feature_name)\n",
    "    correlation_matrix, mask, correlated_pairs, new_df = get_correlated_pairs(df3, threshold)\n",
    "\n",
    "    return correlation_matrix, mask, correlated_pairs, new_df\n",
    "\n",
    "# define controllers\n",
    "style = {'description_width': 'initial'}\n",
    "remove_features_controller = widgets.IntSlider(value=100, min=50, max=400, step=50, continuous_update=False, description=\"Min cities\", style=style)\n",
    "missing_val_controller = widgets.Dropdown(\n",
    "    options=['remove', 'impute'],\n",
    "    value='remove',\n",
    "    description='Treat missing values in rows',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "categorical_controller = widgets.Dropdown(\n",
    "    options=['label_encoding', 'binary_encoding', 'remove'],\n",
    "    value='binary_encoding',\n",
    "    description='Treat categorical features',\n",
    "    disabled=False, \n",
    "    style=style\n",
    ")\n",
    "correlation_features_controller = widgets.FloatSlider(value=0.9, min=0.9, max=0.99, step=0.01, description=\"Threshold\")\n",
    "controller_container = widgets.VBox([remove_features_controller, missing_val_controller, categorical_controller, correlation_features_controller])\n",
    "# initialize the figure\n",
    "correlation_matrix, mask, correlated_pairs, new_df = data_preparation(df, 100, 'remove', 'binary_encoding', 'ez_code', 0.9)\n",
    "pio.templates.default = \"plotly_white\"\n",
    "layout = go.Layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    margin=go.layout.Margin(l=50, r=50, b=100, t=100, pad=4),\n",
    ")\n",
    "fig = go.FigureWidget(\n",
    "    data=go.Heatmap(\n",
    "    z=correlation_matrix.mask(mask),\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale=px.colors.diverging.RdBu,\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "),\n",
    "layout=layout)\n",
    "\n",
    "# bind controllers and figure\n",
    "def response(change):\n",
    "    print(\"updating\")\n",
    "    df1 = remove_features(df, remove_features_controller.value)\n",
    "    df2 = treat_missing_values(df1, missing_val_controller.value)\n",
    "    df3 = treat_cat_features(df2, categorical_controller.value, 'ez_code')\n",
    "    correlation_matrix, mask, correlated_pairs, new_df = get_correlated_pairs(df3, correlation_features_controller.value)\n",
    "    with fig.batch_update():\n",
    "        fig.data[0].z = correlation_matrix.mask(mask)\n",
    "        fig.data[0].x = correlation_matrix.columns\n",
    "        fig.data[0].y = correlation_matrix.columns\n",
    "\n",
    "remove_features_controller.observe(response, names=\"value\")\n",
    "missing_val_controller.observe(response, names=\"value\")\n",
    "categorical_controller.observe(response, names=\"value\")\n",
    "correlation_features_controller.observe(response, names=\"value\")\n",
    "figure1 = widgets.VBox([controller_container, fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3bc1ba-ba1e-447a-b8d6-22eeb8ecb7f4",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Once the data have been cleaned, we can start the clustering. Tune the different clustering settings to get the optimal result.\n",
    "It is possible to change the following parameters:\n",
    "- Feature normalization: choose between `min_max` or `normal` scaling.\n",
    "- Clustering method: choose between classic `k-means clustering` or `weighted k-means clustering`. In the latter case, you must define the weights for each feature. The weights are used when updating the cluster centroids.\n",
    "- Optimal number of clusters: use the elbow method to determine what is the optimal number of clusters. The figure below charts the clusters inertia versus the number of clusters. The optimal number of cluster is the one that minimizes the inertia as well as the number of clusters. The red dot on the graph indicates the current choice of number of clusters (`k`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89828440-eca5-4a76-9eea-196b047c04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting features (From 0 to 4 are cities infos, e.g.s city code)\n",
    "# print(new_df.columns)\n",
    "final_df = new_df[\n",
    "    ['urau_code', 'urau_name', '_wgs84x', '_wgs84y', 'city_area_ha',\n",
    "       'dem_mean', 'imd_percent_2018', 'treecover_percent_2018', \n",
    "    #    'class_11100',\n",
    "    #    'class_11210', 'class_11220', 'class_11230', 'class_11240',\n",
    "    #    'class_11300', 'class_12210', 'class_12230', 'class_12300',\n",
    "    #    'class_12400', 'class_13100', 'class_13300', 'class_13400',\n",
    "    #    'class_14100', 'class_14200', 'class_21000', 'class_22000',\n",
    "    #    'class_23000', 'class_24000', 'class_25000', 'class_31000',\n",
    "    #    'class_32000', 'class_33000', 'class_40000', 'class_50000',\n",
    "      #  'urban_blue_percent', \n",
    "       'urban_green_percent', 'avg_2m_temp_kelvin_2018',\n",
    "       'number_of_summer_days_2018', 'number_of_tropical_nights_2018',\n",
    "       'utci_heat_nights_2018', \n",
    "       #'coastal_city', \n",
    "       'de1028i_2018',\n",
    "       'de1055i_2018']]\n",
    "# discard non-numeric features\n",
    "features = final_df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b517b-6cae-429b-bb57-df1441abb921",
   "metadata": {},
   "source": [
    "### WeightedKmean, revised Kmeans Algorithm to consider the weight \n",
    "The weights are considered during the update of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33920e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.weighted_kmeans import WeightedKMeans\n",
    "# ## initialize all weights to 1\n",
    "feature_weights = [1]*len(features.columns)\n",
    "# ## change weight of selected features\n",
    "weight = 1.5\n",
    "idx = features.columns.get_loc('treecover_percent_2018')\n",
    "feature_weights[idx] = weight\n",
    "# ## Normalize weights\n",
    "feature_weights /= np.sum(feature_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ffcdb-cd4b-43b8-996a-464ce3c50eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "#TODO add select multiple widget\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "k_max = 30\n",
    "def scaler(features, method='min_max'):\n",
    "    if method=='min_max':\n",
    "        scaler_f = MinMaxScaler(feature_range=(0, 1))\n",
    "    else:\n",
    "        scaler_f = StandardScaler()\n",
    "    normalized_features = scaler_f.fit_transform(features)\n",
    "    return normalized_features, scaler_f\n",
    "def clustering(normalized_features, method='normal', k=8, feature_weights=None):\n",
    "    if(method=='normal'):\n",
    "        kmeans = KMeans(n_init=10)\n",
    "        kmeans.random_state=42\n",
    "        kmeans.n_clusters = k\n",
    "        kmeans.max_iter = 100\n",
    "        kmeans.fit(normalized_features)\n",
    "    else:\n",
    "        kmeans = WeightedKMeans()\n",
    "        kmeans.random_state=42\n",
    "        kmeans.n_clusters = k\n",
    "        kmeans.max_iter = 100\n",
    "        kmeans.fit(normalized_features, feature_weights)\n",
    "    return kmeans\n",
    "def elbow_method(normalized_features, k_max=k_max, method='normal', feature_weights=None):\n",
    "    inertia = []\n",
    "    k_range = range(1, k_max)\n",
    "    if(method=='normal'):\n",
    "        kmeans = KMeans(n_init=10)\n",
    "        kmeans.random_state=42\n",
    "        for k in k_range:\n",
    "            kmeans.n_clusters = k\n",
    "            kmeans.fit(normalized_features)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "    else:\n",
    "        kmeans = WeightedKMeans()\n",
    "        kmeans.random_state=42\n",
    "        for k in k_range:\n",
    "            kmeans.n_clusters = k\n",
    "            kmeans.fit(normalized_features, feature_weights)\n",
    "            inertia.append(kmeans.inertia_)\n",
    "    return inertia\n",
    "\n",
    "#set up controllers\n",
    "k_controller = widgets.IntSlider(value=8, min=1, max=k_max-1, step=1, description=\"Optimal k\", continuous_update=False, style=style)\n",
    "norm_method_controller = widgets.Dropdown(\n",
    "    options=['min_max', 'standard'],\n",
    "    value='min_max',\n",
    "    description='Choose normalization method',\n",
    "    disabled=False, \n",
    "    style=style\n",
    ")\n",
    "cluster_method_controller = widgets.Dropdown(\n",
    "    options=['normal', 'weighted'],\n",
    "    value='normal',\n",
    "    description='Choose clustering method',\n",
    "    disabled=False, \n",
    "    style=style\n",
    ")\n",
    "\n",
    "controller_container2 = widgets.VBox([norm_method_controller, cluster_method_controller, k_controller])\n",
    "# initialize the figure\n",
    "normalized_features,fitted_scaler = scaler(features, method='min_max')\n",
    "inertia = elbow_method(normalized_features, method='normal')\n",
    "cluster_df = final_df[['urau_code', 'urau_name', '_wgs84x', '_wgs84y']]\n",
    "cluster_df['cluster'] =  clustering(normalized_features, \n",
    "                            method=cluster_method_controller.value, \n",
    "                            k=k_controller.value, feature_weights=feature_weights).labels_\n",
    "layout = go.Layout(\n",
    "    autosize=False,\n",
    "    width=500,\n",
    "    height=500,\n",
    "    margin=go.layout.Margin(l=50, r=50, b=100, t=100, pad=4),\n",
    ")\n",
    "fig2 = go.FigureWidget(\n",
    "    data=go.Scatter(name=\"Inertia\", x=list(range(1, k_max)), y=inertia),\n",
    "    layout=layout)\n",
    "fig2.add_trace(go.Scatter(name=\"Number of clusters\", x=[8], y=[inertia[7]], mode='markers'))\n",
    "fig2.update_xaxes(title_text='number of clusters')\n",
    "fig2.update_yaxes(title_text='inertia')\n",
    "# bind controllers and figure\n",
    "def response2(change):\n",
    "    print(\"start update\")\n",
    "    normalized_features, _ = scaler(features, method=norm_method_controller.value)\n",
    "    inertia2 = elbow_method(normalized_features, method=cluster_method_controller.value, feature_weights=feature_weights)\n",
    "    cluster_df['cluster'] =  clustering(normalized_features, \n",
    "                                method=cluster_method_controller.value, \n",
    "                                k=k_controller.value, feature_weights=feature_weights).labels_\n",
    "    with fig2.batch_update():\n",
    "        fig2.data[0].y = inertia2\n",
    "        fig2.data[1].x = [k_controller.value]\n",
    "        fig2.data[1].y = [inertia2[k_controller.value-1]]\n",
    "    print(\"finish update\")\n",
    "\n",
    "norm_method_controller.observe(response2, names=\"value\")\n",
    "cluster_method_controller.observe(response2, names=\"value\")\n",
    "k_controller.observe(response2, names=\"value\")\n",
    "figure2 = widgets.VBox([controller_container2, fig2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba924093",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e045f-1e7a-4091-a1fd-f542cc1b884a",
   "metadata": {},
   "source": [
    "## Results interpretation\n",
    "### Geographical interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c57190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, GeoData, LegendControl\n",
    "import geopandas\n",
    "\n",
    "m = Map(center=(46.91, 7.43), zoom=4, layout=widgets.Layout(height='500px'))\n",
    "cmap_hex = [\"#FF5733\", \"#C70039\", \"#900C3F\", \"#581845\", \"#FFC300\", \"#DAF7A6\", \"#884EA0\", \"#2471A3\", \"#2E4053\", \"#1B4F72\", \"#186A3B\", \"#A569BD\", \"#5D6D7E\", \"#AEB6BF\", \"#212F3D\", \"#283747\", \"#1B2631\", \"#515A5A\", \"#17202A\", \"#6E2C00\"]\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    cluster_df, geometry=geopandas.points_from_xy(cluster_df._wgs84x, cluster_df._wgs84y))\n",
    "for idx in range(0, np.max(cluster_df.cluster)+1):\n",
    "    geo_data = GeoData(geo_dataframe = gdf[gdf.cluster==idx],\n",
    "        point_style={'color': cmap_hex[idx], 'fillColor': cmap_hex[idx], 'radius': 5, 'fillOpacity': 0.8,'weight': 3},\n",
    "        name = 'Release')\n",
    "    m.add(geo_data)\n",
    "\n",
    "legend = LegendControl(dict(zip(range(k_controller.value), cmap_hex[:k_controller.value])), title=\"Legend\", position=\"bottomright\")\n",
    "legend.title = \"Clusters\"\n",
    "m.add(legend)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c7e3a-fca6-400f-a07f-430eaaba3738",
   "metadata": {},
   "source": [
    "### Transformed feature space\n",
    "How 'good' are cities separated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427b37f-b50f-4584-b583-b03f8b9dbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "tsne_features = tsne.fit_transform(normalized_features)\n",
    "# # Create a 3D scatter plot for t-SNE visualization\n",
    "layout = go.Layout(\n",
    "    showlegend=True,\n",
    "    scene=go.Scene(\n",
    "        xaxis=go.XAxis(title='t-SNE Feature 1'),\n",
    "        yaxis=go.YAxis(title='t-SNE Feature 2'),\n",
    "        zaxis=go.ZAxis(title='t-SNE Feature 3')\n",
    "    ),\n",
    "    margin=go.layout.Margin(l=10, r=10, b=10, t=10, pad=4),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "figure3 = go.Figure(layout=layout)\n",
    "\n",
    "for cluster_label, color in zip(range(k_controller.value), cmap_hex[:k_controller.value]):\n",
    "    # Filter data points belonging to the current cluster\n",
    "    cluster_indices = np.where(cluster_df.cluster == cluster_label)[0]\n",
    "    figure3.add_trace(go.Scatter3d(x=tsne_features[cluster_indices, 0], y=tsne_features[cluster_indices, 1], z=tsne_features[cluster_indices, 2],\n",
    "                                   mode='markers',\n",
    "                                   marker=dict(size=5,\n",
    "                                               color=color,\n",
    "                                               opacity=0.5,)))\n",
    "\n",
    "figure3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb35d0-6a0f-489b-a3e8-51b76de9b061",
   "metadata": {},
   "source": [
    "### Cluster \"profile\"\n",
    "#TODO\n",
    "### Most influencing features\n",
    "Which feature has the most influence on each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7be5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.kmeans_feature_imp import KMeansInterp\n",
    "kmeans_I = KMeansInterp(\n",
    "        n_clusters=k_controller.value,\n",
    "        ordered_feature_names=features.columns.tolist(),\n",
    "        max_iter=300,\n",
    "        feature_importance_method='wcss_min',  # or 'unsup2sup'\n",
    "    ).fit(normalized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7dc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame(index=new_df.columns)\n",
    "for cluster in kmeans_I.feature_importances_:\n",
    "    idxs, vals = kmeans_I.feature_importances_[cluster]\n",
    "    tmp_df = pd.DataFrame(vals, index=idxs, columns=[cluster])\n",
    "    fi_df = pd.merge(fi_df, tmp_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    # autosize=False,\n",
    "    # width=1000,\n",
    "    # height=1000,\n",
    "    margin=go.layout.Margin(l=10, r=10, b=10, t=10, pad=4),\n",
    ")\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "    z=fi_df.to_numpy(),\n",
    "    x=fi_df.columns,\n",
    "    y=fi_df.index,\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "),\n",
    "layout=layout)\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
